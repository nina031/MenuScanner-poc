{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal :\n",
    "\n",
    "* Construction d'un fichier json structurant l'ensemble du menu avec :\n",
    "    * separation en sections (entrées, plats principaux, desserts ...)\n",
    "    * Identification des plats individuels\n",
    "    * Extraction des prix\n",
    "    * Taguer tous les plats selon le regime alimentaire.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.81.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.10.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.11.4-py3-none-any.whl.metadata (66 kB)\n",
      "Collecting sniffio (from openai)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openai)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.33.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Downloading openai-1.81.0-py3-none-any.whl (717 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m717.5/717.5 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading jiter-0.10.0-cp313-cp313-macosx_11_0_arm64.whl (318 kB)\n",
      "Downloading pydantic-2.11.4-py3-none-any.whl (443 kB)\n",
      "Downloading pydantic_core-2.33.2-cp313-cp313-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-inspection, tqdm, sniffio, pydantic-core, jiter, h11, distro, annotated-types, pydantic, httpcore, anyio, httpx, openai\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/13\u001b[0m [openai]12/13\u001b[0m [openai]c]\n",
      "\u001b[1A\u001b[2KSuccessfully installed annotated-types-0.7.0 anyio-4.9.0 distro-1.9.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 jiter-0.10.0 openai-1.81.0 pydantic-2.11.4 pydantic-core-2.33.2 sniffio-1.3.1 tqdm-4.67.1 typing-inspection-0.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: anthropic in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (0.51.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from anthropic) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from anthropic) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from anthropic) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from anthropic) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from anthropic) (2.11.4)\n",
      "Requirement already satisfied: sniffio in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from anthropic) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from anthropic) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpx<1,>=0.25.0->anthropic) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "from anthropic import Anthropic\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv() \n",
    "\n",
    "claude_api_key = os.environ.get('claude_api_key')\n",
    "gpt_api_key = os.environ.get('gpt_api_key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "marcello_ocr_result_path = \"../data/raw_extracted/Azure_doc_intelligence/result_OCR_marcello.txt\"\n",
    "prima_ocr_result_path = \"../data/raw_extracted/Azure_doc_intelligence/result_OCR_prima.txt\"\n",
    "output_dir = '../data/segmented_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structuration du resultat de l'OCR en utilisant le LLM Claude Haiku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_menu_openai(ocr_text, prompt,api_key):\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": prompt},\n",
    "                {\"role\": \"user\", \"content\": ocr_text}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_menu_claude(ocr_text, prompt,api_key):\n",
    "    client = Anthropic(api_key=api_key)\n",
    "    try:\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",  # ou \"claude-3-7-sonnet-20250219\" pour le plus récent\n",
    "            max_tokens=8192,\n",
    "            temperature=0,\n",
    "            system=prompt,  # Le prompt système va ici, pas dans les messages\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": ocr_text}\n",
    "            ]\n",
    "        )\n",
    "        return response.content[0].text\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_response(response):\n",
    "    \"\"\"\n",
    "    Version minimaliste pour nettoyer les réponses OpenAI.\n",
    "    \n",
    "    Args:\n",
    "        response (dict ou str): Réponse de l'API OpenAI\n",
    "        \n",
    "    Returns:\n",
    "        dict: Menu en JSON\n",
    "    \"\"\"\n",
    "    # Convertir en dict si c'est une string\n",
    "    if isinstance(response, str):\n",
    "        response = json.loads(response)\n",
    "        return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition du prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Analyse ce texte OCR de menu et retourne uniquement un JSON valide suivant cette structure:\n",
    "\n",
    "{{\n",
    "  \"menu\": {{\n",
    "    \"sections\": [\n",
    "      {{\n",
    "        \"name\": \"nom_section\",\n",
    "        \"items\": [\n",
    "          {{\n",
    "            \"name\": \"nom_plat\",\n",
    "            \"price\": {{\"value\": 12.50, \"currency\": null}},\n",
    "            \"description\": \"description_complète\",\n",
    "            \"ingredients\": [\"ingrédient1\", \"ingrédient2\"],\n",
    "            \"dietary\": [\"végétarien\"]\n",
    "          }}\n",
    "        ]\n",
    "      }}\n",
    "    ]\n",
    "  }}\n",
    "}}\n",
    "\n",
    "Texte OCR: {ocr_text}\n",
    "\n",
    "Instructions:\n",
    "1. Identifie automatiquement les sections (entrées, plats, desserts, pizzas, boissons, etc.)\n",
    "2. Pour chaque item: nom, prix, description, ingrédients (déduis-les de la description si nécessaire)\n",
    "3. Prix: utilise uniquement €, $, £, CHF pour currency. Si autre chose ou illisible, mets null\n",
    "\n",
    "IMPORTANT - Régimes alimentaires (sois très prudent):\n",
    "- Si tu as un grand doute, laisse dietary vide []\n",
    "- Règles strictes:\n",
    "  * \"végétarien\": AUCUNE viande, poisson, fruits de mer (mais œufs/lait OK)\n",
    "  * \"végétalien\": AUCUN produit animal (pas de viande, poisson, œufs, lait, miel, beurre)\n",
    "  * \"sans_gluten\": AUCUN blé, orge, seigle, avoine (attention aux sauces, panure)\n",
    "  * \"sans_lactose\": AUCUN lait, crème, fromage, beurre, yaourt\n",
    "\n",
    "ATTENTION - VIANDES (jamais végétarien):\n",
    "- Jambon, jambon blanc, jambon cru, prosciutto = VIANDE\n",
    "- Bacon, lardons, pancetta = VIANDE  \n",
    "- Saucisse, chorizo, pepperoni = VIANDE\n",
    "- Salami, coppa, bresaola = VIANDE\n",
    "- Bœuf, porc, agneau, veau = VIANDE\n",
    "- Poulet, canard, dinde = VIANDE\n",
    "\n",
    "Exemples:\n",
    "\n",
    "VÉGÉTARIEN + VÉGÉTALIEN:\n",
    "- Salade verte simple = [\"végétarien\", \"végétalien\"]\n",
    "- Légumes grillés sans sauce = [\"végétarien\", \"végétalien\"] \n",
    "- Frites maison = [\"végétarien\", \"végétalien\"]\n",
    "- Soupe de légumes (bouillon végétal) = [\"végétarien\", \"végétalien\"]\n",
    "\n",
    "VÉGÉTARIEN SEULEMENT:\n",
    "- Pâtes au beurre = [\"végétarien\"] (beurre = produit laitier)\n",
    "- Pizza margherita = [\"végétarien\"] (fromage = produit laitier)\n",
    "- Omelette = [\"végétarien\"] (œufs OK pour végétarien)\n",
    "\n",
    "SANS GLUTEN SEULEMENT:\n",
    "- Steak grillé nature = [\"sans_gluten\"] (pas de panure ni sauce)\n",
    "- Salade de riz = [\"sans_gluten\"] (riz OK)\n",
    "- Poisson grillé nature = [\"sans_gluten\"]\n",
    "\n",
    "SANS LACTOSE SEULEMENT:\n",
    "- Pâtes à l'huile d'olive = [\"sans_lactose\"] (pas de beurre/fromage)\n",
    "- Viande grillée nature = [\"sans_lactose\"]\n",
    "\n",
    "COMBINAISONS:\n",
    "- Salade de quinoa aux légumes = [\"végétarien\", \"végétalien\", \"sans_gluten\", \"sans_lactose\"]\n",
    "- Riz sauté aux légumes = [\"végétarien\", \"végétalien\", \"sans_gluten\", \"sans_lactose\"]\n",
    "- Steak frites = [\"sans_gluten\", \"sans_lactose\"] (si frites maison)\n",
    "IMPORTANT : il faut inclure le resultat complet (tous les elements presents dans le texte OCR)\n",
    "Retourne uniquement le JSON, sans texte additionnel.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on Marcello menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(marcello_ocr_result_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    marcello_ocr_result = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_marcello = analyze_menu_openai(marcello_ocr_result, prompt,openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_marcello = analyze_menu_claude(marcello_ocr_result, prompt,claude_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_result_marcello = clean_response(result_marcello)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enregistrement du resultat dans un fichier\n",
    "with open(output_dir + \"/result_LLM_sonnet_marcello\", \"w\") as file: \n",
    "        json.dump(cleaned_result_marcello, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on Prima lova menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(prima_ocr_result_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    prima_ocr_result = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_prima = analyze_menu_openai(prima_ocr_result, prompt,openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_prima = analyze_menu_claude(prima_ocr_result, prompt,claude_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_result_prima = clean_response(result_prima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enregistrement du resultat dans un fichier\n",
    "with open(output_dir + \"/result_LLM_sonnet_prima\", \"w\") as file: \n",
    "        json.dump(cleaned_result_prima, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Le modele Haiku de Claude ne peut pas gerer les gros menu a cause de la limite de tokens\n",
    "* gpt-4o-mini n'est pas limité par la limite de token dans notre cas d'usage mais le temps de traitement dure plus de 2 min pour les gros menu et 22 secondes pour les petit menu (2 fois plus lent que claude haiku)\n",
    "* resultats legerement superieur pour claude haiku\n",
    "* Le modele presentant les meilleurs ressultats pour le moment c'est le modele sonnet de Claude (plus rapide, plus performant, peut supporter la limite de toekn pour notre cas d'usage)\n",
    "* Point d'attention : bien que sonnet presente des bons resultats, les modeles sont trop lent. Trouver une solution.\n",
    "* Piste a explorer : \n",
    "*   Tester le streaming, refaire le prompt pour que le modele nous envoie un format jsonL pour pas avoir de probleme de format de la reponse. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traitement par sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from anthropic import Anthropic\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "claude_api_key = os.environ.get('claude_api_key')\n",
    "\n",
    "client = Anthropic(api_key=claude_api_key)\n",
    "\n",
    "def call_claude(text: str, prompt: str) -> str:\n",
    "   response = client.messages.create(\n",
    "       model=\"claude-3-5-sonnet-20241022\",\n",
    "       max_tokens=2000,\n",
    "       temperature=0,\n",
    "       system=prompt,\n",
    "       messages=[{\"role\": \"user\", \"content\": text}]\n",
    "   )\n",
    "   return response.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titre du menu: RIMA FABRICA\n",
      "Sections détectées: ['ANTIPASTI', 'INSALATE', 'CARNE', 'PASTA', 'DOLCI', 'PRZE']\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Prompt pour détecter les sections ET le titre\n",
    "DETECT_SECTIONS_PROMPT = \"\"\"Analyse ce texte OCR de menu et retourne uniquement un JSON avec les noms des sections et le titre du restaurant/menu.\n",
    "\n",
    "Format EXACT:\n",
    "{\n",
    " \"menu_title\": \"Nom du Restaurant/Menu\",\n",
    " \"sections\": [\"SECTION1\", \"SECTION2\", \"SECTION3\"]\n",
    "}\n",
    "\n",
    "Instructions:\n",
    "1. Identifie le titre/nom du restaurant (généralement en haut du menu)\n",
    "2. Identifie automatiquement toutes les sections du menu (entrées, plats, desserts, pizzas, boissons, etc.)\n",
    "3. GARDE EXACTEMENT les noms de sections comme ils apparaissent dans le texte OCR - ne les traduis PAS, ne les modifie PAS\n",
    "4. Ne retourne QUE le JSON, rien d'autre\"\"\"\n",
    "\n",
    "def detect_sections_and_title(ocr_text: str) -> tuple:\n",
    "   response = call_claude(ocr_text, DETECT_SECTIONS_PROMPT)\n",
    "   \n",
    "   try:\n",
    "       # Essayer de parser directement\n",
    "       data = json.loads(response)\n",
    "       return data[\"sections\"], data[\"menu_title\"]\n",
    "   except json.JSONDecodeError:\n",
    "       # Chercher le JSON dans la réponse\n",
    "       json_match = re.search(r'\\{[^}]*\"menu_title\"[^}]*\"sections\"[^}]*\\}', response)\n",
    "       if json_match:\n",
    "           try:\n",
    "               json_str = json_match.group()\n",
    "               print(f\"JSON extrait: {json_str}\")\n",
    "               data = json.loads(json_str)\n",
    "               return data[\"sections\"], data[\"menu_title\"]\n",
    "           except:\n",
    "               print(\"❌ Erreur parsing JSON extrait\")\n",
    "       \n",
    "       # Fallback\n",
    "       sections = re.findall(r'\"([A-Z][A-Z\\sÀ-Ü]*)\"', response)\n",
    "       valid_sections = [s for s in sections if len(s.strip()) >= 3]\n",
    "       return valid_sections, \"Menu\"\n",
    "\n",
    "# Test\n",
    "sections, menu_title = detect_sections_and_title(prima_ocr_result)\n",
    "print(\"Titre du menu:\", menu_title)\n",
    "print(\"Sections détectées:\", sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"sections\": [\n",
      "    {\n",
      "      \"name\": \"ANTIPASTI\",\n",
      "      \"content\": \"1, 2, 3 Bruschettas\\n8-\\nPoire pochée, gorgonzola, noisettes/coeur de Burrata,\\nasperges/Bufala, Speck, crème de balsamique et tomates confites\\nLégumes grillés marinés Balsamico di Modena\\n7-\\nPoivrons, tomates, aubergines, courgettes, artichauts\\nGorgonzola fondante, à la louche carrément !\\n7-\\nChutney de poire et balsamique blanc\\nMozzarella di Bufala 125g/250g\\n7,5/12,5-\\nServie avec nos huiles d'olive maison\\nMozzarella di Burrata 125g/200g\\n8/11,5-\\nServie avec nos huiles d'olive maison\\nRital cochonnailles\\n9-\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"INSALATE\",\n",
      "      \"content\": \"Ave Caesar\\n12,5-\\nSalade romaine, sauce caesar, pancetta grillée, suprême de\\nvolaille, oeuf poché, croûtons, parmesan\\nLa Prima Donna\\n12,5-\\nToast chaud de gorgonzola, noisettes grillées, chiffonade de\\nCoppa, balsamique blanc, miel et huile d'olive\\nVerdure Burrata\\n13-\\nLégumes marinés, Burrata, pignons de pain, pesto Genovese\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"CARNE\",\n",
      "      \"content\": \"Crazy Milanaise\\n20-\\nEscalope de veau panée (noisettes, ail, oignons frits, thym,\\norigan, persil)\\nTartare de boeuf\\n14-\\nBoeuf Charolais au couteau, huile de noisette, tomates confites,\\npersil, cornichons, pignons de pin, câpres, parmesan, frites\\nmaison, salade\\nCarpaccio de boeuf\\n12,50-\\nHuile d'olive, balsamique, roquette, parmesan, frites maison,\\nsalade\\nVitello tonnato\\n12-\\nQuasi de veau en fines tranches, marinade au thon et aux\\nanchois, huile d'olive\\nRIMA\\nFABRICA\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"PASTA\",\n",
      "      \"content\": \"Caponata della Nonna\\n12-\\nraisins secs, coeur de Burrata\\nRigatoni all'amatriciana\\n12-\\nTomates San Marzano D.O.P, pancetta, piments, fromage Pecorino di\\nAmatrice\\nFusilli giganti alla Puttanesca\\n12-\\nSauce piquante, tomates cerises, anchois, parmesan, mozzarella di\\nBufala\\nLa pâte à la truffe\\n14-\\nCrème de truffe noire, crème, parmesan\\nPesto forever\\n14-\\nLe vrai pesto de Genovese, coeur de Burrata\\nGnocchi ai funghi, guanciale\\n13-\\nCrème de champignons, joue de porc grillée, parmesan\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"DOLCI\",\n",
      "      \"content\": \"Pizzetta tressée au Nutella\\n7-\\nTiramisu\\n7-\\nBaba au rhum, mousseline au limoncello\\n6-\\nMousse al cioccolato della Nonna\\n6-\\nPanna cotta, soupe de fruits de saison\\n6-\\nCafé gourmand\\n8-\\nGorgonzola fondante, à la louche carrément !\\nChutney de poire et balsamique blanc\\n7-\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"PRZE\",\n",
      "      \"content\": \"Primargherita\\n11-\\nTomates San Marzano D.O.P, mozzarella, Bufala, basilic, olives\\nTaggiasche\\nRegina\\n12,5-\\nTomates San Marzano D.O.P, mozzarella Fior di Latte, champignons,\\njambon Rostello, olives\\nTartufata\\n15-\\nMozzarella Fior di Latte, champignons, crème de truffe noire,\\nparmesan, ciboulette\\nVeggie lovers\\n15-\\nTomates San Marzano D.O.P, mozzarella Fior di Latte, légumes\\nmarinés, Bufala, crème de balsamique, roquette\\nCinque formaggi, quatre fromages +1\\n15,5-\\nCrème, mozzarella Fior di Latte, gorgonzola, Pecorino, Provolone,\\ncoeur de Burrata, roquette\\nÔ mon cochon\\n16-\\nCrème, mozzarella Fior di Latte, champignons, Burrata, Guanciale\\n(joue de porc)\\nLa Materazzi\\n14-\\nTomates San Marzano D.O.P, mozzarella Fior di Latte, Speck,\\npignons de pin, parmesan, crème de balsamique, coup de tête de\\nroquette\\nCicciolina (c'est chaud !!! )\\n13-\\nTomates San Marzano D.O.P, mozzarella Fior di Latte, poivrons,\\nSpianata Calabrese (saucisson piquant) , roquette, crème de\\nbalsamique\\nCourgettes with love\\n12,5-\\nCrème, mozzarella Fior di Latte, courgettes, crème de balsamique,\\ncoeur de Burrata, graines de courge, fromage Pecorino\\nRed Hot Chili Biquette\\n13,5-\\nCrème, mozzarella Fior di Latte, salame piccante, ricotta de\\nbrebis, oignons rouges\\nBuffalo Bufala\\n15-\\nTomates San Marzano D.O.P, mozzarella Fior di Latte, boeuf,\\nBufala, basilic, œuf poché\\nLa Reine des truffes\\n18-\\nMozzarella Fior di Latte, crème de truffe noire, champignons,\\njambon aux truffes, pignons de pin\\n* Consultez notre carte des glaces\\n\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Étape 2 - Extraire le contenu d'une section (sans le nom) - Version corrigée\n",
    "import re\n",
    "\n",
    "def extract_section_content(ocr_text: str, section_name: str, all_sections: list) -> str:\n",
    "    lines = ocr_text.split('\\n')\n",
    "    content = []\n",
    "    capturing = False\n",
    "    \n",
    "    for line in lines:\n",
    "        # Vérifier si c'est le début de notre section (mot entier avec frontières)\n",
    "        if re.search(r'\\b' + re.escape(section_name.upper()) + r'\\b', line.upper()):\n",
    "            capturing = True\n",
    "            # Ne pas ajouter la ligne avec le nom de section\n",
    "            continue\n",
    "        elif capturing:\n",
    "            # Arrêter si on trouve une autre section de la liste détectée (mot entier)\n",
    "            if any(re.search(r'\\b' + re.escape(s.upper()) + r'\\b', line.upper()) for s in all_sections if s != section_name):\n",
    "                break\n",
    "            content.append(line)\n",
    "    \n",
    "    return '\\n'.join(content)\n",
    "\n",
    "# Test - Extraire toutes les sections dans un JSON\n",
    "sections_with_content = []\n",
    "\n",
    "for section in sections:\n",
    "    content = extract_section_content(prima_ocr_result, section, sections)\n",
    "    sections_with_content.append({\n",
    "        \"name\": section,\n",
    "        \"content\": content\n",
    "    })\n",
    "\n",
    "# Afficher le résultat JSON\n",
    "result = {\"sections\": sections_with_content}\n",
    "print(json.dumps(result, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🍽️ Début de l'analyse des sections\n",
      "==================================================\n",
      "[1/6] Traitement: ANTIPASTI\n",
      "    ✅ Terminé en 10.48s\n",
      "[2/6] Traitement: INSALATE\n",
      "    ✅ Terminé en 5.50s\n",
      "[3/6] Traitement: CARNE\n",
      "    ✅ Terminé en 7.58s\n",
      "[4/6] Traitement: PASTA\n",
      "    ✅ Terminé en 8.29s\n",
      "[5/6] Traitement: DOLCI\n",
      "    ✅ Terminé en 9.21s\n",
      "[6/6] Traitement: PRZE\n",
      "    ✅ Terminé en 18.28s\n",
      "==================================================\n",
      "⏱️ Temps total: 59.36s\n",
      "📊 Moyenne par section: 9.89s\n",
      "💾 Menu complet sauvegardé: ../data/sections_analyzed/menu_complet.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Étape 3 - Analyser chaque section avec temps de traitement\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "\n",
    "def clean_section_name_for_filename(section_name: str) -> str:\n",
    "    \"\"\"Nettoie le nom de section pour créer un nom de fichier valide.\"\"\"\n",
    "    # Enlever les caractères spéciaux et espaces\n",
    "    clean_name = re.sub(r'[^\\w\\s-]', '', section_name)\n",
    "    # Remplacer les espaces par des underscores\n",
    "    clean_name = re.sub(r'\\s+', '_', clean_name)\n",
    "    # Convertir en minuscules\n",
    "    clean_name = clean_name.lower().strip('_')\n",
    "    return clean_name\n",
    "\n",
    "def analyze_section(section_content: str, section_name: str) -> dict:\n",
    "    # Prompt modifié pour corriger les noms de sections\n",
    "    ANALYZE_SECTION_PROMPT = f\"\"\"Analyse cette section de menu nommée \"{section_name}\" et retourne uniquement un JSON valide suivant cette structure:\n",
    "\n",
    "{{\n",
    "  \"name\": \"nom_section_corrigé\",\n",
    "  \"items\": [\n",
    "    {{\n",
    "      \"name\": \"nom_plat\",\n",
    "      \"price\": {{\"value\": 12.50, \"currency\": \"€\"}},\n",
    "      \"description\": \"description_complète\",\n",
    "      \"ingredients\": [\"ingrédient1\", \"ingrédient2\"],\n",
    "      \"dietary\": [\"végétarien\"]\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Instructions:\n",
    "1. CORRIGE les erreurs OCR évidentes dans le nom de section \"{section_name}\":\n",
    "   - \"PRZE\" → \"PIZZE\"\n",
    "   - \"DOLC\" → \"DOLCI\"\n",
    "   - \"ANTPASTI\" → \"ANTIPASTI\"\n",
    "   - \"NSALATE\" → \"INSALATE\"\n",
    "   - \"CARNE\" → garde \"CARNE\" (correct)\n",
    "   - \"PASTA\" → garde \"PASTA\" (correct)\n",
    "   - etc.\n",
    "   Utilise le nom corrigé dans le champ \"name\" du JSON\n",
    "2. Pour chaque item: nom, prix, description, ingrédients (déduis-les de la description si nécessaire)\n",
    "3. Prix: utilise uniquement €, $, £, CHF pour currency. Si autre chose ou illisible, mets null\n",
    "4. DÉTECTION ET TRADUCTION DE LANGUE:\n",
    "   - Détecte la langue majoritaire du menu\n",
    "   - Si langue du menu = français → PAS de traduction\n",
    "   - Si langue du menu = langue avec même alphabet que l'utilisateur → traduis les descriptions MAIS garde les spécialités/ingrédients authentiques en langue originale\n",
    "   - Si langue du menu = langue avec alphabet différent de l'utilisateur → TRADUIS TOUT car l'utilisateur ne peut pas lire ces caractères\n",
    "   \n",
    "    Logique par type d'alphabet:\n",
    "   \n",
    "   MÊME FAMILLE D'ALPHABET (garde les spécialités):\n",
    "   - Latin vers Latin: Italien→Français, Espagnol→Anglais, etc.\n",
    "   - Cyrillique vers Cyrillique: Russe→Bulgare, etc.\n",
    "   - Arabe vers Arabe: Arabe→Persan, etc.\n",
    "   \n",
    "   ALPHABETS DIFFÉRENTS (traduis tout):\n",
    "   - Latin vers Chinois: \"Carbonara\" → \"卡邦纳拉意面\"\n",
    "   - Chinois vers Latin: \"宫保鸡丁\" → \"Poulet Gong Bao\"\n",
    "   - Arabe vers Latin: \"كباب\" → \"Kebab\"\n",
    "   - Japonais vers Latin: \"寿司\" → \"Sushi\"\n",
    "   \n",
    "   Exemples de spécialités à GARDER (même alphabet):\n",
    "   - Italien→Français: garde \"mozzarella di bufala\", \"parmigiano\"\n",
    "   - Français→Anglais: garde \"coq au vin\", \"bouillabaisse\"\n",
    "   - Espagnol→Italien: garde \"jamón ibérico\", \"paella\"\n",
    "\n",
    "IMPORTANT - Régimes alimentaires (sois très prudent):\n",
    "- Si tu as un grand doute, laisse dietary vide []\n",
    "- Règles strictes:\n",
    "  * \"végétarien\": AUCUNE viande, poisson, fruits de mer (mais œufs/lait OK)\n",
    "  * \"végétalien\": AUCUN produit animal (pas de viande, poisson, œufs, lait, miel, beurre)\n",
    "  * \"sans_gluten\": AUCUN blé, orge, seigle, avoine (attention aux sauces, panure)\n",
    "  * \"sans_lactose\": AUCUN lait, crème, fromage, beurre, yaourt\n",
    "\n",
    "ATTENTION - VIANDES (jamais végétarien):\n",
    "- Jambon, jambon blanc, jambon cru, prosciutto = VIANDE\n",
    "- Bacon, lardons, pancetta = VIANDE  \n",
    "- Saucisse, chorizo, pepperoni = VIANDE\n",
    "- Salami, coppa, bresaola = VIANDE\n",
    "- Bœuf, porc, agneau, veau = VIANDE\n",
    "- Poulet, canard, dinde = VIANDE\n",
    "\n",
    "IMPORTANT: Inclus TOUS les éléments présents dans cette section.\n",
    "Retourne UNIQUEMENT le JSON, sans texte additionnel.\"\"\"\n",
    "\n",
    "    response = call_claude(section_content, ANALYZE_SECTION_PROMPT)\n",
    "    \n",
    "    try:\n",
    "        return json.loads(response)\n",
    "    except json.JSONDecodeError as e:\n",
    "        # Fallback avec le bon nom de section\n",
    "        return {\"name\": section_name, \"items\": []}\n",
    "\n",
    "# Créer le dossier de sortie pour les sections\n",
    "sections_output_dir = '../data/sections_analyzed'\n",
    "os.makedirs(sections_output_dir, exist_ok=True)\n",
    "\n",
    "# Variables pour le suivi du temps total\n",
    "total_start_time = time.time()\n",
    "all_sections_data = []\n",
    "\n",
    "print(\"🍽️ Début de l'analyse des sections\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Traitement complet - boucle sur le JSON des sections\n",
    "for i, section_data in enumerate(sections_with_content, 1):\n",
    "    section_name = section_data[\"name\"]\n",
    "    section_content = section_data[\"content\"]\n",
    "    \n",
    "    print(f\"[{i}/{len(sections_with_content)}] Traitement: {section_name}\")\n",
    "    \n",
    "    # Mesurer le temps de traitement de cette section\n",
    "    section_start_time = time.time()\n",
    "    \n",
    "    # Passer le nom de section à la fonction\n",
    "    analyzed = analyze_section(section_content, section_name)\n",
    "    \n",
    "    section_end_time = time.time()\n",
    "    section_duration = section_end_time - section_start_time\n",
    "    \n",
    "    print(f\"    ✅ Terminé en {section_duration:.2f}s\")\n",
    "    \n",
    "    # Récupérer le nom corrigé du JSON retourné\n",
    "    corrected_section_name = analyzed.get(\"name\", section_name)\n",
    "    \n",
    "    # Enregistrer avec le nom corrigé\n",
    "    clean_filename = clean_section_name_for_filename(corrected_section_name)\n",
    "    section_filepath = os.path.join(sections_output_dir, f\"{clean_filename}.json\")\n",
    "    \n",
    "    with open(section_filepath, 'w', encoding='utf-8') as file:\n",
    "        json.dump(analyzed, file, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    all_sections_data.append(analyzed)\n",
    "\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Calcul du temps total\n",
    "total_end_time = time.time()\n",
    "total_duration = total_end_time - total_start_time\n",
    "\n",
    "print(f\"⏱️ Temps total: {total_duration:.2f}s\")\n",
    "print(f\"📊 Moyenne par section: {total_duration/len(sections_with_content):.2f}s\")\n",
    "\n",
    "# Résultat final\n",
    "menu = {\n",
    "    \"menu\": {\n",
    "        \"name\": menu_title,\n",
    "        \"sections\": all_sections_data\n",
    "    }\n",
    "}\n",
    "\n",
    "# Enregistrer le menu complet\n",
    "final_menu_path = os.path.join(sections_output_dir, \"menu_complet.json\")\n",
    "with open(final_menu_path, 'w', encoding='utf-8') as file:\n",
    "    json.dump(menu, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"💾 Menu complet sauvegardé: {final_menu_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
