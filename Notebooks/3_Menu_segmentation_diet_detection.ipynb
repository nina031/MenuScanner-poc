{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal :\n",
    "\n",
    "* Construction d'un fichier json structurant l'ensemble du menu avec :\n",
    "    * separation en sections (entrées, plats principaux, desserts ...)\n",
    "    * Identification des plats individuels\n",
    "    * Extraction des prix\n",
    "    * Taguer tous les plats selon le regime alimentaire.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.81.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.10.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.11.4-py3-none-any.whl.metadata (66 kB)\n",
      "Collecting sniffio (from openai)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openai)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.33.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Downloading openai-1.81.0-py3-none-any.whl (717 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m717.5/717.5 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading jiter-0.10.0-cp313-cp313-macosx_11_0_arm64.whl (318 kB)\n",
      "Downloading pydantic-2.11.4-py3-none-any.whl (443 kB)\n",
      "Downloading pydantic_core-2.33.2-cp313-cp313-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-inspection, tqdm, sniffio, pydantic-core, jiter, h11, distro, annotated-types, pydantic, httpcore, anyio, httpx, openai\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/13\u001b[0m [openai]12/13\u001b[0m [openai]c]\n",
      "\u001b[1A\u001b[2KSuccessfully installed annotated-types-0.7.0 anyio-4.9.0 distro-1.9.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 jiter-0.10.0 openai-1.81.0 pydantic-2.11.4 pydantic-core-2.33.2 sniffio-1.3.1 tqdm-4.67.1 typing-inspection-0.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: anthropic in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (0.51.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from anthropic) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from anthropic) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from anthropic) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from anthropic) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from anthropic) (2.11.4)\n",
      "Requirement already satisfied: sniffio in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from anthropic) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from anthropic) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpx<1,>=0.25.0->anthropic) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "from anthropic import Anthropic\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv() \n",
    "\n",
    "claude_api_key = os.environ.get('claude_api_key')\n",
    "gpt_api_key = os.environ.get('gpt_api_key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "marcello_ocr_result_path = \"../data/raw_extracted/Azure_doc_intelligence/result_OCR_marcello.txt\"\n",
    "prima_ocr_result_path = \"../data/raw_extracted/Azure_doc_intelligence/result_OCR_prima.txt\"\n",
    "output_dir = '../data/segmented_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structuration du resultat de l'OCR en utilisant le LLM Claude Haiku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_menu_openai(ocr_text, prompt,api_key):\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": prompt},\n",
    "                {\"role\": \"user\", \"content\": ocr_text}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_menu_claude(ocr_text, prompt,api_key):\n",
    "    client = Anthropic(api_key=api_key)\n",
    "    try:\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",  # ou \"claude-3-7-sonnet-20250219\" pour le plus récent\n",
    "            max_tokens=8192,\n",
    "            temperature=0,\n",
    "            system=prompt,  # Le prompt système va ici, pas dans les messages\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": ocr_text}\n",
    "            ]\n",
    "        )\n",
    "        return response.content[0].text\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_response(response):\n",
    "    \"\"\"\n",
    "    Version minimaliste pour nettoyer les réponses OpenAI.\n",
    "    \n",
    "    Args:\n",
    "        response (dict ou str): Réponse de l'API OpenAI\n",
    "        \n",
    "    Returns:\n",
    "        dict: Menu en JSON\n",
    "    \"\"\"\n",
    "    # Convertir en dict si c'est une string\n",
    "    if isinstance(response, str):\n",
    "        response = json.loads(response)\n",
    "        return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition du prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Analyse ce texte OCR de menu et retourne uniquement un JSON valide suivant cette structure:\n",
    "\n",
    "{{\n",
    "  \"menu\": {{\n",
    "    \"sections\": [\n",
    "      {{\n",
    "        \"name\": \"nom_section\",\n",
    "        \"items\": [\n",
    "          {{\n",
    "            \"name\": \"nom_plat\",\n",
    "            \"price\": {{\"value\": 12.50, \"currency\": null}},\n",
    "            \"description\": \"description_complète\",\n",
    "            \"ingredients\": [\"ingrédient1\", \"ingrédient2\"],\n",
    "            \"dietary\": [\"végétarien\"]\n",
    "          }}\n",
    "        ]\n",
    "      }}\n",
    "    ]\n",
    "  }}\n",
    "}}\n",
    "\n",
    "Texte OCR: {ocr_text}\n",
    "\n",
    "Instructions:\n",
    "1. Identifie automatiquement les sections (entrées, plats, desserts, pizzas, boissons, etc.)\n",
    "2. Pour chaque item: nom, prix, description, ingrédients (déduis-les de la description si nécessaire)\n",
    "3. Prix: utilise uniquement €, $, £, CHF pour currency. Si autre chose ou illisible, mets null\n",
    "\n",
    "IMPORTANT - Régimes alimentaires (sois très prudent):\n",
    "- Si tu as un grand doute, laisse dietary vide []\n",
    "- Règles strictes:\n",
    "  * \"végétarien\": AUCUNE viande, poisson, fruits de mer (mais œufs/lait OK)\n",
    "  * \"végétalien\": AUCUN produit animal (pas de viande, poisson, œufs, lait, miel, beurre)\n",
    "  * \"sans_gluten\": AUCUN blé, orge, seigle, avoine (attention aux sauces, panure)\n",
    "  * \"sans_lactose\": AUCUN lait, crème, fromage, beurre, yaourt\n",
    "\n",
    "ATTENTION - VIANDES (jamais végétarien):\n",
    "- Jambon, jambon blanc, jambon cru, prosciutto = VIANDE\n",
    "- Bacon, lardons, pancetta = VIANDE  \n",
    "- Saucisse, chorizo, pepperoni = VIANDE\n",
    "- Salami, coppa, bresaola = VIANDE\n",
    "- Bœuf, porc, agneau, veau = VIANDE\n",
    "- Poulet, canard, dinde = VIANDE\n",
    "\n",
    "Exemples:\n",
    "\n",
    "VÉGÉTARIEN + VÉGÉTALIEN:\n",
    "- Salade verte simple = [\"végétarien\", \"végétalien\"]\n",
    "- Légumes grillés sans sauce = [\"végétarien\", \"végétalien\"] \n",
    "- Frites maison = [\"végétarien\", \"végétalien\"]\n",
    "- Soupe de légumes (bouillon végétal) = [\"végétarien\", \"végétalien\"]\n",
    "\n",
    "VÉGÉTARIEN SEULEMENT:\n",
    "- Pâtes au beurre = [\"végétarien\"] (beurre = produit laitier)\n",
    "- Pizza margherita = [\"végétarien\"] (fromage = produit laitier)\n",
    "- Omelette = [\"végétarien\"] (œufs OK pour végétarien)\n",
    "\n",
    "SANS GLUTEN SEULEMENT:\n",
    "- Steak grillé nature = [\"sans_gluten\"] (pas de panure ni sauce)\n",
    "- Salade de riz = [\"sans_gluten\"] (riz OK)\n",
    "- Poisson grillé nature = [\"sans_gluten\"]\n",
    "\n",
    "SANS LACTOSE SEULEMENT:\n",
    "- Pâtes à l'huile d'olive = [\"sans_lactose\"] (pas de beurre/fromage)\n",
    "- Viande grillée nature = [\"sans_lactose\"]\n",
    "\n",
    "COMBINAISONS:\n",
    "- Salade de quinoa aux légumes = [\"végétarien\", \"végétalien\", \"sans_gluten\", \"sans_lactose\"]\n",
    "- Riz sauté aux légumes = [\"végétarien\", \"végétalien\", \"sans_gluten\", \"sans_lactose\"]\n",
    "- Steak frites = [\"sans_gluten\", \"sans_lactose\"] (si frites maison)\n",
    "IMPORTANT : il faut inclure le resultat complet (tous les elements presents dans le texte OCR)\n",
    "Retourne uniquement le JSON, sans texte additionnel.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on Marcello menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Classiques\\nVERDURA\\n12,90\\ntomate\\', aubergines grillées, poivrons grillés, champignons\\nde Paris, oignons rouges, olives Leccine, huile de basilic, mozzarella\\nfdl\\nMIELINA\\n12.90\\ncrème, chèvre, miel, mozzarella fdl\\'\\nTARTUFO\\n14.90\\ncrème de truffes et champignons, mozzarella fdl\\'\\n(après cuisson, roquette, bresaola, copeaux de parmesan AOC)\\nMARGHERITA\\n9.90\\ntomate\\', olives Leccine, huile de basilic, mozzarella fdl\\'\\nREGINA\\n11,90\\ntomate\\', jambon blanc italien, champignons de Paris,\\nolives Leccine, huile de basilic, mozzarella fdl\\'\\nPARMA\\n14,90\\ntomate @ huile de basilic, mozzarella fdl\\' (après cuisson,\\nroquette, jambon de Parme, burrata, tomates cerises,\\ncopeaux de parmesan AOC, crème de balsamique)\\nQUATTRO FORMAGGI\\n12,00\\ntomate\\', taleggio AOC, gorgonzola DOP,\\nparmigiano AOC, mozzarella fdl\\'\\nCALZONE CLASSICA\\n12,90\\ntomate\\', jambon blanc italien, oeuf, champignons de Paris,\\nhuile de basilic, mozzarella fdl\"\\nMARCELLO\\n14.90\\ncrème, gorgonzola DOP, coppa, huile de basilic, mozzarella fdl\\'\\n(après cuisson, jambon de Parme)\\nQUATTRO STAGIONI\\n13.90\\ntomate\\', jambon blanc, artichauts à la romaine,\\naubergines grillées, poivrons grillés,\\noignons rouges, huile de basilic, mozzarella fdl\\'\\n*Mozzarella fior di latte Orchidea.\\n\"Tomate pleins champs de coopérative italienne.\\nSpianata calabrera : charcuterie italienne, type chorizo\\nBresaola : viande de boeuf salée et assaisonnée, consommée crue\\nTaille unique 29cm.\\n'"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(marcello_ocr_result_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    marcello_ocr_result = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_marcello = analyze_menu_openai(marcello_ocr_result, prompt,openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_marcello = analyze_menu_claude(marcello_ocr_result, prompt,claude_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_result_marcello = clean_response(result_marcello)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enregistrement du resultat dans un fichier\n",
    "with open(output_dir + \"/result_LLM_sonnet_marcello\", \"w\") as file: \n",
    "        json.dump(cleaned_result_marcello, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on Prima lova menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(prima_ocr_result_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    prima_ocr_result = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_prima = analyze_menu_openai(prima_ocr_result, prompt,openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_prima = analyze_menu_claude(prima_ocr_result, prompt,claude_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_result_prima = clean_response(result_prima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enregistrement du resultat dans un fichier\n",
    "with open(output_dir + \"/result_LLM_sonnet_prima\", \"w\") as file: \n",
    "        json.dump(cleaned_result_prima, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Le modele Haiku de Claude ne peut pas gerer les gros menu a cause de la limite de tokens\n",
    "* gpt-4o-mini n'est pas limité par la limite de token dans notre cas d'usage mais le temps de traitement dure plus de 2 min pour les gros menu et 22 secondes pour les petit menu (2 fois plus lent que claude haiku)\n",
    "* resultats legerement superieur pour claude haiku\n",
    "* Le modele presentant les meilleurs ressultats pour le moment c'est le modele sonnet de Claude (plus rapide, plus performant, peut supporter la limite de toekn pour notre cas d'usage)\n",
    "* Point d'attention : bien que sonnet presente des bons resultats, les modeles sont trop lent. Trouver une solution.\n",
    "* Piste a explorer : \n",
    "*   Tester le streaming, refaire le prompt pour que le modele nous envoie un format jsonL pour pas avoir de probleme de format de la reponse. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
