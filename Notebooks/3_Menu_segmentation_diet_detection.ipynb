{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal :\n",
    "\n",
    "* Construction d'un fichier json structurant l'ensemble du menu avec :\n",
    "    * separation en sections (entr√©es, plats principaux, desserts ...)\n",
    "    * Identification des plats individuels\n",
    "    * Extraction des prix\n",
    "    * Taguer tous les plats selon le regime alimentaire.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.81.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.10.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.11.4-py3-none-any.whl.metadata (66 kB)\n",
      "Collecting sniffio (from openai)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openai)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.33.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Downloading openai-1.81.0-py3-none-any.whl (717 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m717.5/717.5 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading jiter-0.10.0-cp313-cp313-macosx_11_0_arm64.whl (318 kB)\n",
      "Downloading pydantic-2.11.4-py3-none-any.whl (443 kB)\n",
      "Downloading pydantic_core-2.33.2-cp313-cp313-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-inspection, tqdm, sniffio, pydantic-core, jiter, h11, distro, annotated-types, pydantic, httpcore, anyio, httpx, openai\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13/13\u001b[0m [openai]12/13\u001b[0m [openai]c]\n",
      "\u001b[1A\u001b[2KSuccessfully installed annotated-types-0.7.0 anyio-4.9.0 distro-1.9.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 jiter-0.10.0 openai-1.81.0 pydantic-2.11.4 pydantic-core-2.33.2 sniffio-1.3.1 tqdm-4.67.1 typing-inspection-0.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: anthropic in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (0.51.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from anthropic) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from anthropic) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from anthropic) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from anthropic) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from anthropic) (2.11.4)\n",
      "Requirement already satisfied: sniffio in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from anthropic) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from anthropic) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpx<1,>=0.25.0->anthropic) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "from anthropic import Anthropic\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv() \n",
    "\n",
    "claude_api_key = os.environ.get('claude_api_key')\n",
    "gpt_api_key = os.environ.get('gpt_api_key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "marcello_ocr_result_path = \"../data/raw_extracted/Azure_doc_intelligence/result_OCR_marcello.txt\"\n",
    "prima_ocr_result_path = \"../data/raw_extracted/Azure_doc_intelligence/result_OCR_prima.txt\"\n",
    "output_dir = '../data/segmented_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structuration du resultat de l'OCR en utilisant le LLM Claude Haiku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_menu_openai(ocr_text, prompt,api_key):\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": prompt},\n",
    "                {\"role\": \"user\", \"content\": ocr_text}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_menu_claude(ocr_text, prompt,api_key):\n",
    "    client = Anthropic(api_key=api_key)\n",
    "    try:\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",  # ou \"claude-3-7-sonnet-20250219\" pour le plus r√©cent\n",
    "            max_tokens=8192,\n",
    "            temperature=0,\n",
    "            system=prompt,  # Le prompt syst√®me va ici, pas dans les messages\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": ocr_text}\n",
    "            ]\n",
    "        )\n",
    "        return response.content[0].text\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_response(response):\n",
    "    \"\"\"\n",
    "    Version minimaliste pour nettoyer les r√©ponses OpenAI.\n",
    "    \n",
    "    Args:\n",
    "        response (dict ou str): R√©ponse de l'API OpenAI\n",
    "        \n",
    "    Returns:\n",
    "        dict: Menu en JSON\n",
    "    \"\"\"\n",
    "    # Convertir en dict si c'est une string\n",
    "    if isinstance(response, str):\n",
    "        response = json.loads(response)\n",
    "        return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition du prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Analyse ce texte OCR de menu et retourne uniquement un JSON valide suivant cette structure:\n",
    "\n",
    "{{\n",
    "  \"menu\": {{\n",
    "    \"sections\": [\n",
    "      {{\n",
    "        \"name\": \"nom_section\",\n",
    "        \"items\": [\n",
    "          {{\n",
    "            \"name\": \"nom_plat\",\n",
    "            \"price\": {{\"value\": 12.50, \"currency\": null}},\n",
    "            \"description\": \"description_compl√®te\",\n",
    "            \"ingredients\": [\"ingr√©dient1\", \"ingr√©dient2\"],\n",
    "            \"dietary\": [\"v√©g√©tarien\"]\n",
    "          }}\n",
    "        ]\n",
    "      }}\n",
    "    ]\n",
    "  }}\n",
    "}}\n",
    "\n",
    "Texte OCR: {ocr_text}\n",
    "\n",
    "Instructions:\n",
    "1. Identifie automatiquement les sections (entr√©es, plats, desserts, pizzas, boissons, etc.)\n",
    "2. Pour chaque item: nom, prix, description, ingr√©dients (d√©duis-les de la description si n√©cessaire)\n",
    "3. Prix: utilise uniquement ‚Ç¨, $, ¬£, CHF pour currency. Si autre chose ou illisible, mets null\n",
    "\n",
    "IMPORTANT - R√©gimes alimentaires (sois tr√®s prudent):\n",
    "- Si tu as un grand doute, laisse dietary vide []\n",
    "- R√®gles strictes:\n",
    "  * \"v√©g√©tarien\": AUCUNE viande, poisson, fruits de mer (mais ≈ìufs/lait OK)\n",
    "  * \"v√©g√©talien\": AUCUN produit animal (pas de viande, poisson, ≈ìufs, lait, miel, beurre)\n",
    "  * \"sans_gluten\": AUCUN bl√©, orge, seigle, avoine (attention aux sauces, panure)\n",
    "  * \"sans_lactose\": AUCUN lait, cr√®me, fromage, beurre, yaourt\n",
    "\n",
    "ATTENTION - VIANDES (jamais v√©g√©tarien):\n",
    "- Jambon, jambon blanc, jambon cru, prosciutto = VIANDE\n",
    "- Bacon, lardons, pancetta = VIANDE  \n",
    "- Saucisse, chorizo, pepperoni = VIANDE\n",
    "- Salami, coppa, bresaola = VIANDE\n",
    "- B≈ìuf, porc, agneau, veau = VIANDE\n",
    "- Poulet, canard, dinde = VIANDE\n",
    "\n",
    "Exemples:\n",
    "\n",
    "V√âG√âTARIEN + V√âG√âTALIEN:\n",
    "- Salade verte simple = [\"v√©g√©tarien\", \"v√©g√©talien\"]\n",
    "- L√©gumes grill√©s sans sauce = [\"v√©g√©tarien\", \"v√©g√©talien\"] \n",
    "- Frites maison = [\"v√©g√©tarien\", \"v√©g√©talien\"]\n",
    "- Soupe de l√©gumes (bouillon v√©g√©tal) = [\"v√©g√©tarien\", \"v√©g√©talien\"]\n",
    "\n",
    "V√âG√âTARIEN SEULEMENT:\n",
    "- P√¢tes au beurre = [\"v√©g√©tarien\"] (beurre = produit laitier)\n",
    "- Pizza margherita = [\"v√©g√©tarien\"] (fromage = produit laitier)\n",
    "- Omelette = [\"v√©g√©tarien\"] (≈ìufs OK pour v√©g√©tarien)\n",
    "\n",
    "SANS GLUTEN SEULEMENT:\n",
    "- Steak grill√© nature = [\"sans_gluten\"] (pas de panure ni sauce)\n",
    "- Salade de riz = [\"sans_gluten\"] (riz OK)\n",
    "- Poisson grill√© nature = [\"sans_gluten\"]\n",
    "\n",
    "SANS LACTOSE SEULEMENT:\n",
    "- P√¢tes √† l'huile d'olive = [\"sans_lactose\"] (pas de beurre/fromage)\n",
    "- Viande grill√©e nature = [\"sans_lactose\"]\n",
    "\n",
    "COMBINAISONS:\n",
    "- Salade de quinoa aux l√©gumes = [\"v√©g√©tarien\", \"v√©g√©talien\", \"sans_gluten\", \"sans_lactose\"]\n",
    "- Riz saut√© aux l√©gumes = [\"v√©g√©tarien\", \"v√©g√©talien\", \"sans_gluten\", \"sans_lactose\"]\n",
    "- Steak frites = [\"sans_gluten\", \"sans_lactose\"] (si frites maison)\n",
    "IMPORTANT : il faut inclure le resultat complet (tous les elements presents dans le texte OCR)\n",
    "Retourne uniquement le JSON, sans texte additionnel.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on Marcello menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(marcello_ocr_result_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    marcello_ocr_result = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_marcello = analyze_menu_openai(marcello_ocr_result, prompt,openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_marcello = analyze_menu_claude(marcello_ocr_result, prompt,claude_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_result_marcello = clean_response(result_marcello)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enregistrement du resultat dans un fichier\n",
    "with open(output_dir + \"/result_LLM_sonnet_marcello\", \"w\") as file: \n",
    "        json.dump(cleaned_result_marcello, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on Prima lova menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(prima_ocr_result_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    prima_ocr_result = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_prima = analyze_menu_openai(prima_ocr_result, prompt,openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_prima = analyze_menu_claude(prima_ocr_result, prompt,claude_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_result_prima = clean_response(result_prima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enregistrement du resultat dans un fichier\n",
    "with open(output_dir + \"/result_LLM_sonnet_prima\", \"w\") as file: \n",
    "        json.dump(cleaned_result_prima, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Le modele Haiku de Claude ne peut pas gerer les gros menu a cause de la limite de tokens\n",
    "* gpt-4o-mini n'est pas limit√© par la limite de token dans notre cas d'usage mais le temps de traitement dure plus de 2 min pour les gros menu et 22 secondes pour les petit menu (2 fois plus lent que claude haiku)\n",
    "* resultats legerement superieur pour claude haiku\n",
    "* Le modele presentant les meilleurs ressultats pour le moment c'est le modele sonnet de Claude (plus rapide, plus performant, peut supporter la limite de toekn pour notre cas d'usage)\n",
    "* Point d'attention : bien que sonnet presente des bons resultats, les modeles sont trop lent. Trouver une solution.\n",
    "* Piste a explorer : \n",
    "*   Tester le streaming, refaire le prompt pour que le modele nous envoie un format jsonL pour pas avoir de probleme de format de la reponse. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traitement par sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from anthropic import Anthropic\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "claude_api_key = os.environ.get('claude_api_key')\n",
    "\n",
    "client = Anthropic(api_key=claude_api_key)\n",
    "\n",
    "def call_claude(text: str, prompt: str) -> str:\n",
    "   response = client.messages.create(\n",
    "       model=\"claude-3-5-sonnet-20241022\",\n",
    "       max_tokens=2000,\n",
    "       temperature=0,\n",
    "       system=prompt,\n",
    "       messages=[{\"role\": \"user\", \"content\": text}]\n",
    "   )\n",
    "   return response.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titre du menu: RIMA FABRICA\n",
      "Sections d√©tect√©es: ['ANTIPASTI', 'INSALATE', 'CARNE', 'PASTA', 'DOLCI', 'PRZE']\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Prompt pour d√©tecter les sections ET le titre\n",
    "DETECT_SECTIONS_PROMPT = \"\"\"Analyse ce texte OCR de menu et retourne uniquement un JSON avec les noms des sections et le titre du restaurant/menu.\n",
    "\n",
    "Format EXACT:\n",
    "{\n",
    " \"menu_title\": \"Nom du Restaurant/Menu\",\n",
    " \"sections\": [\"SECTION1\", \"SECTION2\", \"SECTION3\"]\n",
    "}\n",
    "\n",
    "Instructions:\n",
    "1. Identifie le titre/nom du restaurant (g√©n√©ralement en haut du menu)\n",
    "2. Identifie automatiquement toutes les sections du menu (entr√©es, plats, desserts, pizzas, boissons, etc.)\n",
    "3. GARDE EXACTEMENT les noms de sections comme ils apparaissent dans le texte OCR - ne les traduis PAS, ne les modifie PAS\n",
    "4. Ne retourne QUE le JSON, rien d'autre\"\"\"\n",
    "\n",
    "def detect_sections_and_title(ocr_text: str) -> tuple:\n",
    "   response = call_claude(ocr_text, DETECT_SECTIONS_PROMPT)\n",
    "   \n",
    "   try:\n",
    "       # Essayer de parser directement\n",
    "       data = json.loads(response)\n",
    "       return data[\"sections\"], data[\"menu_title\"]\n",
    "   except json.JSONDecodeError:\n",
    "       # Chercher le JSON dans la r√©ponse\n",
    "       json_match = re.search(r'\\{[^}]*\"menu_title\"[^}]*\"sections\"[^}]*\\}', response)\n",
    "       if json_match:\n",
    "           try:\n",
    "               json_str = json_match.group()\n",
    "               print(f\"JSON extrait: {json_str}\")\n",
    "               data = json.loads(json_str)\n",
    "               return data[\"sections\"], data[\"menu_title\"]\n",
    "           except:\n",
    "               print(\"‚ùå Erreur parsing JSON extrait\")\n",
    "       \n",
    "       # Fallback\n",
    "       sections = re.findall(r'\"([A-Z][A-Z\\s√Ä-√ú]*)\"', response)\n",
    "       valid_sections = [s for s in sections if len(s.strip()) >= 3]\n",
    "       return valid_sections, \"Menu\"\n",
    "\n",
    "# Test\n",
    "sections, menu_title = detect_sections_and_title(prima_ocr_result)\n",
    "print(\"Titre du menu:\", menu_title)\n",
    "print(\"Sections d√©tect√©es:\", sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"sections\": [\n",
      "    {\n",
      "      \"name\": \"ANTIPASTI\",\n",
      "      \"content\": \"1, 2, 3 Bruschettas\\n8-\\nPoire poch√©e, gorgonzola, noisettes/coeur de Burrata,\\nasperges/Bufala, Speck, cr√®me de balsamique et tomates confites\\nL√©gumes grill√©s marin√©s Balsamico di Modena\\n7-\\nPoivrons, tomates, aubergines, courgettes, artichauts\\nGorgonzola fondante, √† la louche carr√©ment !\\n7-\\nChutney de poire et balsamique blanc\\nMozzarella di Bufala 125g/250g\\n7,5/12,5-\\nServie avec nos huiles d'olive maison\\nMozzarella di Burrata 125g/200g\\n8/11,5-\\nServie avec nos huiles d'olive maison\\nRital cochonnailles\\n9-\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"INSALATE\",\n",
      "      \"content\": \"Ave Caesar\\n12,5-\\nSalade romaine, sauce caesar, pancetta grill√©e, supr√™me de\\nvolaille, oeuf poch√©, cro√ªtons, parmesan\\nLa Prima Donna\\n12,5-\\nToast chaud de gorgonzola, noisettes grill√©es, chiffonade de\\nCoppa, balsamique blanc, miel et huile d'olive\\nVerdure Burrata\\n13-\\nL√©gumes marin√©s, Burrata, pignons de pain, pesto Genovese\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"CARNE\",\n",
      "      \"content\": \"Crazy Milanaise\\n20-\\nEscalope de veau pan√©e (noisettes, ail, oignons frits, thym,\\norigan, persil)\\nTartare de boeuf\\n14-\\nBoeuf Charolais au couteau, huile de noisette, tomates confites,\\npersil, cornichons, pignons de pin, c√¢pres, parmesan, frites\\nmaison, salade\\nCarpaccio de boeuf\\n12,50-\\nHuile d'olive, balsamique, roquette, parmesan, frites maison,\\nsalade\\nVitello tonnato\\n12-\\nQuasi de veau en fines tranches, marinade au thon et aux\\nanchois, huile d'olive\\nRIMA\\nFABRICA\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"PASTA\",\n",
      "      \"content\": \"Caponata della Nonna\\n12-\\nraisins secs, coeur de Burrata\\nRigatoni all'amatriciana\\n12-\\nTomates San Marzano D.O.P, pancetta, piments, fromage Pecorino di\\nAmatrice\\nFusilli giganti alla Puttanesca\\n12-\\nSauce piquante, tomates cerises, anchois, parmesan, mozzarella di\\nBufala\\nLa p√¢te √† la truffe\\n14-\\nCr√®me de truffe noire, cr√®me, parmesan\\nPesto forever\\n14-\\nLe vrai pesto de Genovese, coeur de Burrata\\nGnocchi ai funghi, guanciale\\n13-\\nCr√®me de champignons, joue de porc grill√©e, parmesan\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"DOLCI\",\n",
      "      \"content\": \"Pizzetta tress√©e au Nutella\\n7-\\nTiramisu\\n7-\\nBaba au rhum, mousseline au limoncello\\n6-\\nMousse al cioccolato della Nonna\\n6-\\nPanna cotta, soupe de fruits de saison\\n6-\\nCaf√© gourmand\\n8-\\nGorgonzola fondante, √† la louche carr√©ment !\\nChutney de poire et balsamique blanc\\n7-\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"PRZE\",\n",
      "      \"content\": \"Primargherita\\n11-\\nTomates San Marzano D.O.P, mozzarella, Bufala, basilic, olives\\nTaggiasche\\nRegina\\n12,5-\\nTomates San Marzano D.O.P, mozzarella Fior di Latte, champignons,\\njambon Rostello, olives\\nTartufata\\n15-\\nMozzarella Fior di Latte, champignons, cr√®me de truffe noire,\\nparmesan, ciboulette\\nVeggie lovers\\n15-\\nTomates San Marzano D.O.P, mozzarella Fior di Latte, l√©gumes\\nmarin√©s, Bufala, cr√®me de balsamique, roquette\\nCinque formaggi, quatre fromages +1\\n15,5-\\nCr√®me, mozzarella Fior di Latte, gorgonzola, Pecorino, Provolone,\\ncoeur de Burrata, roquette\\n√î mon cochon\\n16-\\nCr√®me, mozzarella Fior di Latte, champignons, Burrata, Guanciale\\n(joue de porc)\\nLa Materazzi\\n14-\\nTomates San Marzano D.O.P, mozzarella Fior di Latte, Speck,\\npignons de pin, parmesan, cr√®me de balsamique, coup de t√™te de\\nroquette\\nCicciolina (c'est chaud !!! )\\n13-\\nTomates San Marzano D.O.P, mozzarella Fior di Latte, poivrons,\\nSpianata Calabrese (saucisson piquant) , roquette, cr√®me de\\nbalsamique\\nCourgettes with love\\n12,5-\\nCr√®me, mozzarella Fior di Latte, courgettes, cr√®me de balsamique,\\ncoeur de Burrata, graines de courge, fromage Pecorino\\nRed Hot Chili Biquette\\n13,5-\\nCr√®me, mozzarella Fior di Latte, salame piccante, ricotta de\\nbrebis, oignons rouges\\nBuffalo Bufala\\n15-\\nTomates San Marzano D.O.P, mozzarella Fior di Latte, boeuf,\\nBufala, basilic, ≈ìuf poch√©\\nLa Reine des truffes\\n18-\\nMozzarella Fior di Latte, cr√®me de truffe noire, champignons,\\njambon aux truffes, pignons de pin\\n* Consultez notre carte des glaces\\n\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: √âtape 2 - Extraire le contenu d'une section (sans le nom) - Version corrig√©e\n",
    "import re\n",
    "\n",
    "def extract_section_content(ocr_text: str, section_name: str, all_sections: list) -> str:\n",
    "    lines = ocr_text.split('\\n')\n",
    "    content = []\n",
    "    capturing = False\n",
    "    \n",
    "    for line in lines:\n",
    "        # V√©rifier si c'est le d√©but de notre section (mot entier avec fronti√®res)\n",
    "        if re.search(r'\\b' + re.escape(section_name.upper()) + r'\\b', line.upper()):\n",
    "            capturing = True\n",
    "            # Ne pas ajouter la ligne avec le nom de section\n",
    "            continue\n",
    "        elif capturing:\n",
    "            # Arr√™ter si on trouve une autre section de la liste d√©tect√©e (mot entier)\n",
    "            if any(re.search(r'\\b' + re.escape(s.upper()) + r'\\b', line.upper()) for s in all_sections if s != section_name):\n",
    "                break\n",
    "            content.append(line)\n",
    "    \n",
    "    return '\\n'.join(content)\n",
    "\n",
    "# Test - Extraire toutes les sections dans un JSON\n",
    "sections_with_content = []\n",
    "\n",
    "for section in sections:\n",
    "    content = extract_section_content(prima_ocr_result, section, sections)\n",
    "    sections_with_content.append({\n",
    "        \"name\": section,\n",
    "        \"content\": content\n",
    "    })\n",
    "\n",
    "# Afficher le r√©sultat JSON\n",
    "result = {\"sections\": sections_with_content}\n",
    "print(json.dumps(result, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üçΩÔ∏è D√©but de l'analyse des sections\n",
      "==================================================\n",
      "[1/6] Traitement: ANTIPASTI\n",
      "    ‚úÖ Termin√© en 10.48s\n",
      "[2/6] Traitement: INSALATE\n",
      "    ‚úÖ Termin√© en 5.50s\n",
      "[3/6] Traitement: CARNE\n",
      "    ‚úÖ Termin√© en 7.58s\n",
      "[4/6] Traitement: PASTA\n",
      "    ‚úÖ Termin√© en 8.29s\n",
      "[5/6] Traitement: DOLCI\n",
      "    ‚úÖ Termin√© en 9.21s\n",
      "[6/6] Traitement: PRZE\n",
      "    ‚úÖ Termin√© en 18.28s\n",
      "==================================================\n",
      "‚è±Ô∏è Temps total: 59.36s\n",
      "üìä Moyenne par section: 9.89s\n",
      "üíæ Menu complet sauvegard√©: ../data/sections_analyzed/menu_complet.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: √âtape 3 - Analyser chaque section avec temps de traitement\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "\n",
    "def clean_section_name_for_filename(section_name: str) -> str:\n",
    "    \"\"\"Nettoie le nom de section pour cr√©er un nom de fichier valide.\"\"\"\n",
    "    # Enlever les caract√®res sp√©ciaux et espaces\n",
    "    clean_name = re.sub(r'[^\\w\\s-]', '', section_name)\n",
    "    # Remplacer les espaces par des underscores\n",
    "    clean_name = re.sub(r'\\s+', '_', clean_name)\n",
    "    # Convertir en minuscules\n",
    "    clean_name = clean_name.lower().strip('_')\n",
    "    return clean_name\n",
    "\n",
    "def analyze_section(section_content: str, section_name: str) -> dict:\n",
    "    # Prompt modifi√© pour corriger les noms de sections\n",
    "    ANALYZE_SECTION_PROMPT = f\"\"\"Analyse cette section de menu nomm√©e \"{section_name}\" et retourne uniquement un JSON valide suivant cette structure:\n",
    "\n",
    "{{\n",
    "  \"name\": \"nom_section_corrig√©\",\n",
    "  \"items\": [\n",
    "    {{\n",
    "      \"name\": \"nom_plat\",\n",
    "      \"price\": {{\"value\": 12.50, \"currency\": \"‚Ç¨\"}},\n",
    "      \"description\": \"description_compl√®te\",\n",
    "      \"ingredients\": [\"ingr√©dient1\", \"ingr√©dient2\"],\n",
    "      \"dietary\": [\"v√©g√©tarien\"]\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Instructions:\n",
    "1. CORRIGE les erreurs OCR √©videntes dans le nom de section \"{section_name}\":\n",
    "   - \"PRZE\" ‚Üí \"PIZZE\"\n",
    "   - \"DOLC\" ‚Üí \"DOLCI\"\n",
    "   - \"ANTPASTI\" ‚Üí \"ANTIPASTI\"\n",
    "   - \"NSALATE\" ‚Üí \"INSALATE\"\n",
    "   - \"CARNE\" ‚Üí garde \"CARNE\" (correct)\n",
    "   - \"PASTA\" ‚Üí garde \"PASTA\" (correct)\n",
    "   - etc.\n",
    "   Utilise le nom corrig√© dans le champ \"name\" du JSON\n",
    "2. Pour chaque item: nom, prix, description, ingr√©dients (d√©duis-les de la description si n√©cessaire)\n",
    "3. Prix: utilise uniquement ‚Ç¨, $, ¬£, CHF pour currency. Si autre chose ou illisible, mets null\n",
    "4. D√âTECTION ET TRADUCTION DE LANGUE:\n",
    "   - D√©tecte la langue majoritaire du menu\n",
    "   - Si langue du menu = fran√ßais ‚Üí PAS de traduction\n",
    "   - Si langue du menu = langue avec m√™me alphabet que l'utilisateur ‚Üí traduis les descriptions MAIS garde les sp√©cialit√©s/ingr√©dients authentiques en langue originale\n",
    "   - Si langue du menu = langue avec alphabet diff√©rent de l'utilisateur ‚Üí TRADUIS TOUT car l'utilisateur ne peut pas lire ces caract√®res\n",
    "   \n",
    "    Logique par type d'alphabet:\n",
    "   \n",
    "   M√äME FAMILLE D'ALPHABET (garde les sp√©cialit√©s):\n",
    "   - Latin vers Latin: Italien‚ÜíFran√ßais, Espagnol‚ÜíAnglais, etc.\n",
    "   - Cyrillique vers Cyrillique: Russe‚ÜíBulgare, etc.\n",
    "   - Arabe vers Arabe: Arabe‚ÜíPersan, etc.\n",
    "   \n",
    "   ALPHABETS DIFF√âRENTS (traduis tout):\n",
    "   - Latin vers Chinois: \"Carbonara\" ‚Üí \"Âç°ÈÇ¶Á∫≥ÊãâÊÑèÈù¢\"\n",
    "   - Chinois vers Latin: \"ÂÆ´‰øùÈ∏°‰∏Å\" ‚Üí \"Poulet Gong Bao\"\n",
    "   - Arabe vers Latin: \"ŸÉÿ®ÿßÿ®\" ‚Üí \"Kebab\"\n",
    "   - Japonais vers Latin: \"ÂØøÂè∏\" ‚Üí \"Sushi\"\n",
    "   \n",
    "   Exemples de sp√©cialit√©s √† GARDER (m√™me alphabet):\n",
    "   - Italien‚ÜíFran√ßais: garde \"mozzarella di bufala\", \"parmigiano\"\n",
    "   - Fran√ßais‚ÜíAnglais: garde \"coq au vin\", \"bouillabaisse\"\n",
    "   - Espagnol‚ÜíItalien: garde \"jam√≥n ib√©rico\", \"paella\"\n",
    "\n",
    "IMPORTANT - R√©gimes alimentaires (sois tr√®s prudent):\n",
    "- Si tu as un grand doute, laisse dietary vide []\n",
    "- R√®gles strictes:\n",
    "  * \"v√©g√©tarien\": AUCUNE viande, poisson, fruits de mer (mais ≈ìufs/lait OK)\n",
    "  * \"v√©g√©talien\": AUCUN produit animal (pas de viande, poisson, ≈ìufs, lait, miel, beurre)\n",
    "  * \"sans_gluten\": AUCUN bl√©, orge, seigle, avoine (attention aux sauces, panure)\n",
    "  * \"sans_lactose\": AUCUN lait, cr√®me, fromage, beurre, yaourt\n",
    "\n",
    "ATTENTION - VIANDES (jamais v√©g√©tarien):\n",
    "- Jambon, jambon blanc, jambon cru, prosciutto = VIANDE\n",
    "- Bacon, lardons, pancetta = VIANDE  \n",
    "- Saucisse, chorizo, pepperoni = VIANDE\n",
    "- Salami, coppa, bresaola = VIANDE\n",
    "- B≈ìuf, porc, agneau, veau = VIANDE\n",
    "- Poulet, canard, dinde = VIANDE\n",
    "\n",
    "IMPORTANT: Inclus TOUS les √©l√©ments pr√©sents dans cette section.\n",
    "Retourne UNIQUEMENT le JSON, sans texte additionnel.\"\"\"\n",
    "\n",
    "    response = call_claude(section_content, ANALYZE_SECTION_PROMPT)\n",
    "    \n",
    "    try:\n",
    "        return json.loads(response)\n",
    "    except json.JSONDecodeError as e:\n",
    "        # Fallback avec le bon nom de section\n",
    "        return {\"name\": section_name, \"items\": []}\n",
    "\n",
    "# Cr√©er le dossier de sortie pour les sections\n",
    "sections_output_dir = '../data/sections_analyzed'\n",
    "os.makedirs(sections_output_dir, exist_ok=True)\n",
    "\n",
    "# Variables pour le suivi du temps total\n",
    "total_start_time = time.time()\n",
    "all_sections_data = []\n",
    "\n",
    "print(\"üçΩÔ∏è D√©but de l'analyse des sections\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Traitement complet - boucle sur le JSON des sections\n",
    "for i, section_data in enumerate(sections_with_content, 1):\n",
    "    section_name = section_data[\"name\"]\n",
    "    section_content = section_data[\"content\"]\n",
    "    \n",
    "    print(f\"[{i}/{len(sections_with_content)}] Traitement: {section_name}\")\n",
    "    \n",
    "    # Mesurer le temps de traitement de cette section\n",
    "    section_start_time = time.time()\n",
    "    \n",
    "    # Passer le nom de section √† la fonction\n",
    "    analyzed = analyze_section(section_content, section_name)\n",
    "    \n",
    "    section_end_time = time.time()\n",
    "    section_duration = section_end_time - section_start_time\n",
    "    \n",
    "    print(f\"    ‚úÖ Termin√© en {section_duration:.2f}s\")\n",
    "    \n",
    "    # R√©cup√©rer le nom corrig√© du JSON retourn√©\n",
    "    corrected_section_name = analyzed.get(\"name\", section_name)\n",
    "    \n",
    "    # Enregistrer avec le nom corrig√©\n",
    "    clean_filename = clean_section_name_for_filename(corrected_section_name)\n",
    "    section_filepath = os.path.join(sections_output_dir, f\"{clean_filename}.json\")\n",
    "    \n",
    "    with open(section_filepath, 'w', encoding='utf-8') as file:\n",
    "        json.dump(analyzed, file, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    all_sections_data.append(analyzed)\n",
    "\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Calcul du temps total\n",
    "total_end_time = time.time()\n",
    "total_duration = total_end_time - total_start_time\n",
    "\n",
    "print(f\"‚è±Ô∏è Temps total: {total_duration:.2f}s\")\n",
    "print(f\"üìä Moyenne par section: {total_duration/len(sections_with_content):.2f}s\")\n",
    "\n",
    "# R√©sultat final\n",
    "menu = {\n",
    "    \"menu\": {\n",
    "        \"name\": menu_title,\n",
    "        \"sections\": all_sections_data\n",
    "    }\n",
    "}\n",
    "\n",
    "# Enregistrer le menu complet\n",
    "final_menu_path = os.path.join(sections_output_dir, \"menu_complet.json\")\n",
    "with open(final_menu_path, 'w', encoding='utf-8') as file:\n",
    "    json.dump(menu, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"üíæ Menu complet sauvegard√©: {final_menu_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
